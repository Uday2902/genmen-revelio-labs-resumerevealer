{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7739586,"sourceType":"datasetVersion","datasetId":4523601},{"sourceId":7740927,"sourceType":"datasetVersion","datasetId":4524456},{"sourceId":7740978,"sourceType":"datasetVersion","datasetId":4524495},{"sourceId":7741047,"sourceType":"datasetVersion","datasetId":4524543},{"sourceId":7741078,"sourceType":"datasetVersion","datasetId":4524568},{"sourceId":7741125,"sourceType":"datasetVersion","datasetId":4524601}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Company Name : Revelio Labs\n# ResumeRevealer Track: Advanced Resume Parsing Challenge\n\n# Team : GenMen\n\n## Team members : \n### Uday Patel\n### Urvish Patel\n### Naitik Prajapati\n### Prit Patel\n### Kevin Prajapati","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-02T05:17:51.850627Z","iopub.execute_input":"2024-03-02T05:17:51.850994Z","iopub.status.idle":"2024-03-02T05:17:51.892327Z","shell.execute_reply.started":"2024-03-02T05:17:51.850964Z","shell.execute_reply":"2024-03-02T05:17:51.891494Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"/kaggle/input/html-resume/21BCE221_Practical2A_Resume.html\n/kaggle/input/resume-parser/model-last/tokenizer\n/kaggle/input/resume-parser/model-last/meta.json\n/kaggle/input/resume-parser/model-last/config.cfg\n/kaggle/input/resume-parser/model-last/vocab/vectors\n/kaggle/input/resume-parser/model-last/vocab/key2row\n/kaggle/input/resume-parser/model-last/vocab/vectors.cfg\n/kaggle/input/resume-parser/model-last/vocab/strings.json\n/kaggle/input/resume-parser/model-last/vocab/lookups.bin\n/kaggle/input/resume-parser/model-last/ner/model\n/kaggle/input/resume-parser/model-last/ner/moves\n/kaggle/input/resume-parser/model-last/ner/cfg\n/kaggle/input/resume-parser/model-last/transformer/model\n/kaggle/input/resume-parser/model-last/transformer/cfg\n/kaggle/input/resume-parser/model-best/tokenizer\n/kaggle/input/resume-parser/model-best/meta.json\n/kaggle/input/resume-parser/model-best/config.cfg\n/kaggle/input/resume-parser/model-best/vocab/vectors\n/kaggle/input/resume-parser/model-best/vocab/key2row\n/kaggle/input/resume-parser/model-best/vocab/vectors.cfg\n/kaggle/input/resume-parser/model-best/vocab/strings.json\n/kaggle/input/resume-parser/model-best/vocab/lookups.bin\n/kaggle/input/resume-parser/model-best/ner/model\n/kaggle/input/resume-parser/model-best/ner/moves\n/kaggle/input/resume-parser/model-best/ner/cfg\n/kaggle/input/resume-parser/model-best/transformer/model\n/kaggle/input/resume-parser/model-best/transformer/cfg\n/kaggle/input/lstm-onet-model/onet_lstm_model.h5\n/kaggle/input/testing-resume/resume_testing.docx\n/kaggle/input/tech-skill-dataset/TECH_SKILLS.csv\n/kaggle/input/testing-resumes/Uday_Resume.pdf\n/kaggle/input/testing-resumes/Image.pdf\n/kaggle/input/testing-resumes/Word Embeddings.docx\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Downloading spacy library","metadata":{}},{"cell_type":"code","source":"!pip install spacy_transformers\n!pip install -U spacy","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:15:32.052498Z","iopub.execute_input":"2024-03-02T05:15:32.053362Z","iopub.status.idle":"2024-03-02T05:15:57.660794Z","shell.execute_reply.started":"2024-03-02T05:15:32.053326Z","shell.execute_reply":"2024-03-02T05:15:57.659552Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Collecting spacy_transformers\n  Downloading spacy_transformers-1.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\nRequirement already satisfied: spacy<4.0.0,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (3.7.2)\nCollecting transformers<4.37.0,>=3.4.0 (from spacy_transformers)\n  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (2.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (2.4.8)\nCollecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers)\n  Downloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\nRequirement already satisfied: numpy>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from spacy_transformers) (1.26.4)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (8.2.2)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (1.1.2)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (0.9.0)\nRequirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (6.4.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (4.66.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.31.0)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (2.5.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (69.0.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (21.3)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (3.2.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->spacy_transformers) (2024.2.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (0.20.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (0.15.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.37.0,>=3.4.0->spacy_transformers) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (2024.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.7.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.1.8->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.1.4)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (8.1.7)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.5.0->spacy_transformers) (0.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.5.0->spacy_transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->spacy_transformers) (1.3.0)\nDownloading spacy_transformers-1.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.9/197.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading spacy_alignments-0.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: spacy-alignments, transformers, spacy_transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.38.1\n    Uninstalling transformers-4.38.1:\n      Successfully uninstalled transformers-4.38.1\nSuccessfully installed spacy-alignments-0.9.1 spacy_transformers-1.3.4 transformers-4.36.2\nERROR: unknown command \"install -U spacy\" - maybe you meant \"install\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Loading fine tunned model","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom spacy.tokens import DocBin\nfrom tqdm import tqdm\nimport json\n\nnlp= spacy.load('/kaggle/input/resume-parser/model-best')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:16:07.728627Z","iopub.execute_input":"2024-03-02T05:16:07.729614Z","iopub.status.idle":"2024-03-02T05:16:22.206321Z","shell.execute_reply.started":"2024-03-02T05:16:07.729575Z","shell.execute_reply":"2024-03-02T05:16:22.205436Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_pipeline' (0.0.0) was trained with spaCy v3.7.4 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n  warnings.warn(warn_msg)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Using *converapi* to extract text data from various types of resume formats\n\nConvertAPI is a versatile online service that provides file conversion capabilities, allowing users to convert documents between different formats easily. Here's a brief overview of its advantages and potential disadvantages:\n\n### Advantages:\n\n**Format Flexibility**: ConvertAPI supports a wide range of file formats, including popular ones like DOC, PDF, HTML, and various image formats. This flexibility allows users to convert between different document types seamlessly.\n\n**Ease of Use**: The service is designed to be user-friendly, offering a simple API that developers can integrate into their applications. This makes it accessible for both technical and non-technical users.\n\n**Automation**: With ConvertAPI, users can automate the process of converting documents. This is particularly useful for tasks that involve bulk conversions or routine document processing.\n\n**Cloud-Based Service**: Being a cloud-based service, ConvertAPI performs document conversions in the cloud. This can be advantageous for users with limited local resources or those who need to access the service from various locations.\n\n### **Disadvantages**:\n\n**Dependency on External Service**: As a cloud-based service, users depend on ConvertAPI's availability and performance. Any disruptions in the service could impact the ability to perform document conversions.\n\n**Costs**: While ConvertAPI offers a free tier, higher usage or specific features may come with associated costs. Users should be mindful of their usage patterns and the potential costs involved.\n\n**Privacy and Security Concerns**: When dealing with sensitive or confidential documents, users need to consider the privacy and security implications of uploading files to an external service. Ensure that the service complies with data protection regulations and industry standards.\n\n**Limited Customization**: While ConvertAPI provides a convenient way to perform common document conversions, users might find the customization options limited compared to dedicated, locally hosted solutions.\n\nIn summary, ConvertAPI is a valuable tool for document conversion, offering ease of use and format flexibility. However, users should be aware of potential costs, consider privacy and security implications, and evaluate whether the service meets their specific needs.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade convertapi","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:16:28.518742Z","iopub.execute_input":"2024-03-02T05:16:28.519856Z","iopub.status.idle":"2024-03-02T05:16:41.057521Z","shell.execute_reply.started":"2024-03-02T05:16:28.519822Z","shell.execute_reply":"2024-03-02T05:16:41.056469Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: convertapi in /opt/conda/lib/python3.10/site-packages (1.7.0)\nRequirement already satisfied: requests>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from convertapi) (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.4.2->convertapi) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.4.2->convertapi) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.4.2->convertapi) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.4.2->convertapi) (2024.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"import convertapi\n\nconvertapi.api_secret = '89M2Lxyvcz3V7KyY'\n\nresult = convertapi.convert('pdf', { 'File': '/kaggle/input/testing-resume/resume_testing.docx' })\nresult.file.save('/kaggle/working/results.pdf')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:17:08.304781Z","iopub.execute_input":"2024-03-02T05:17:08.305150Z","iopub.status.idle":"2024-03-02T05:17:10.344422Z","shell.execute_reply.started":"2024-03-02T05:17:08.305119Z","shell.execute_reply":"2024-03-02T05:17:10.343522Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/results.pdf'"},"metadata":{}}]},{"cell_type":"code","source":"convertapi.convert('txt', {\n    'File': '/kaggle/working/results.pdf'\n}, from_format = 'pdf').save_files('/kaggle/working/final_results.txt')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:17:11.688992Z","iopub.execute_input":"2024-03-02T05:17:11.689421Z","iopub.status.idle":"2024-03-02T05:17:12.885149Z","shell.execute_reply.started":"2024-03-02T05:17:11.689387Z","shell.execute_reply":"2024-03-02T05:17:12.884235Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/final_results.txt']"},"metadata":{}}]},{"cell_type":"markdown","source":"# Text extractor code without using any API","metadata":{}},{"cell_type":"markdown","source":"### Downloading dependencies","metadata":{}},{"cell_type":"code","source":"!pip install docx2txt\n!pip install PyPDF2\n!pip install bs4\n!apt-get install tesseract-ocr\n!apt-get install libtesseract-dev\n!pip install pytesseract\n!pip install pdf2image\n!apt-get install poppler-utils\n!pip install pdfplumber","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Python script is designed to extract text content from various types of documents, including PDFs, DOCX files, HTML files, and images (JPEG and PNG). It utilizes different libraries for each file type to extract text information.\n\nHere's a brief description of the code:\n\n**Libraries Used**:\n\n**os**: Used for interacting with the operating system, such as extracting file extensions.\n\n**docx2txt**: A library to extract text from DOCX files.\n\n**PyPDF2**: A library to work with PDF files.\n\n**BeautifulSoup**: Used for parsing HTML content.\n\n**PIL (Python Imaging Library)**: Used for working with images.\n\n**pytesseract**: Integrates with Tesseract OCR engine for extracting text from images.\nextract_text Function:\n\nAccepts a file path as input.\nDetermines the file extension using os.path.splitext.\nCalls specific functions based on the file type to extract text.\nFile-Specific Functions:\n\n**PDF (extract_text_from_pdf)**: Uses PyPDF2 to read PDF files page by page and extract text.\n\n**DOCX (extract_text_from_docx)**: Utilizes the docx2txt library to process DOCX files and extract text.\n\n**HTML (extract_text_from_html)**: Parses HTML content using BeautifulSoup and retrieves text.\n\n**Image (extract_text_from_image)**: Uses Tesseract OCR through pytesseract to perform optical character recognition on images and extract text.\nmain Function:\n\nTakes user input for the path of the resume file.\nCalls the extract_text function to extract and print the text content.","metadata":{}},{"cell_type":"code","source":"import os\nimport docx2txt\nimport PyPDF2\nfrom bs4 import BeautifulSoup\nfrom PIL import Image\nimport pytesseract\n\n\ndef extract_text(file_path):\n    _, file_extension = os.path.splitext(file_path)\n\n    if file_extension == '.pdf':\n        return extract_text_from_pdf(file_path)\n    elif file_extension == '.docx':\n        return extract_text_from_docx(file_path)\n    elif file_extension == '.html':\n        return extract_text_from_html(file_path)\n    elif file_extension in ['.jpg', '.png']:\n        return extract_text_from_image(file_path)\n\n    \ndef extract_text_from_pdf(file_path):\n  with open(file_path, \"rb\") as pdf_file:\n    pdf_reader = PyPDF2.PdfReader(pdf_file)\n    text = \"\"\n    for page in pdf_reader.pages:\n        text += page.extract_text()\n\n  print(text)\n\n\ndef extract_text_from_docx(file_path):\n  extracted_text = docx2txt.process(file_path)\n  print(extracted_text)\n\n    \ndef extract_text_from_html(file_path):\n  with open(file_path, \"r\", encoding=\"utf-8\") as html_file:\n    soup = BeautifulSoup(html_file, \"html.parser\")\n    text = soup.get_text()\n\n  print(text)\n\n\ndef extract_text_from_image(file_path):\n    image = Image.open(file_path)\n    text = pytesseract.image_to_string(image)\n    print(text)\n\ndef main():\n    \n    resume_path = input(\"Give the resume path: \")\n    resume_text = extract_text(resume_path)\n    resume_text = extract_text_api(resume_path)c\n    \nmain()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Primary Challenge: \n\nDevelop a comprehensive resume parser, \"ResumeRevealer,\" capable of extracting detailed information from resumes in various formats (PDF, JPG, HTML, DOC, etc.). The parser should accurately classify text into distinct sections (e.g., education, work experience, skills) and sequence them based on dates, where available.","metadata":{}},{"cell_type":"markdown","source":"# Fine tunned Spacy parser model, classifying extracted text into distinct sections like education, work experience, skills, where available.","metadata":{}},{"cell_type":"markdown","source":"## **Skill Extraction Challenge**: \n\nImplement an advanced feature in \"ResumeRevealer\" that mines detailed skills and competencies from project descriptions and position roles within the resume, highlighting the candidate's specific abilities and expertise. Abstractive skill extraction is a bonus.","metadata":{}},{"cell_type":"code","source":"file_path = \"/kaggle/working/final_results.txt\"\nwith open(file_path, \"r\") as file:\n    text = file.read()\ntexter = text\ndoc=nlp(texter)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:17:15.347159Z","iopub.execute_input":"2024-03-02T05:17:15.347850Z","iopub.status.idle":"2024-03-02T05:17:16.971313Z","shell.execute_reply.started":"2024-03-02T05:17:15.347819Z","shell.execute_reply":"2024-03-02T05:17:16.970495Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"final_ans = {}\nfor ent in doc.ents:\n    if ent.label_ in final_ans:\n        final_ans[ent.label_].append(ent.text)\n    else:\n        final_ans[ent.label_] = [ent.text]\n\ndef print_formatted_dict(data):\n    for key, values in data.items():\n        print(f\"{key}:\",'\\n')\n        if isinstance(values, list):\n            for value in values:\n                print(f\"  - {value}\",'\\n')\n        else:\n            print(f\"  - {values}\",'\\n')\n\nprint_formatted_dict(final_ans)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:17:19.242240Z","iopub.execute_input":"2024-03-02T05:17:19.242894Z","iopub.status.idle":"2024-03-02T05:17:19.253060Z","shell.execute_reply.started":"2024-03-02T05:17:19.242862Z","shell.execute_reply":"2024-03-02T05:17:19.252048Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Location: \n\n  - Manhattan \n\n  - New \n\n  - York \n\nDesignation: \n\n  - Software Developer \n\n  - Research Data Engineerin \n\n  - Associate Data Scientist \n\n  - Advisor: \n\n  - Advisor: \n\n  - ACADEMIC ACHIEVEMENTS AND LEADERSHIP EXPERIENCE \n\n  - Events Coordinator \n\n  - Technical Lead \n\nSkills: \n\n  - Python, Django, NodeJS, C & C++, SQL, CI/CD, JavaScript, Big Data (Hadoop, Spark), Cloud (AWS, GCP \n\nCompanies worked at: \n\n  - Robert \n\n  - Bosch \n\n  - Bosch \n\nCollege Name: \n\n  - Leonard N. Stern School of Business \n\n  - New York University \n\n  - Courant and Stern School \n\n  - Indian Institute of Information Technology \n\nDegree: \n\n  - MBA \n\n  - Master of Science, Information Systems- \n\n  - Bachelor of Technology, Computer Science and Engineering \n\n  - Bachelors’ Thesis)- \n\nGraduation Year: \n\n  - 2022 \n\nName: \n\n  - Rakesh Matam \n\n  - Gautam Barua \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Extracting skills from above classified skills section","metadata":{}},{"cell_type":"code","source":"import re\ndef extract_skills_as_string(data):\n    pattern = re.compile(r'[^a-zA-Z0-9\\s,]+')\n    skills = [skill.lower().strip() for skill_list in data.get('Skills', []) for skill in skill_list.split(',')]\n    skills = [pattern.sub('', skill) for skill in skills]\n    return ', '.join(skills)\nskills_string = extract_skills_as_string(final_ans)\nprint(skills_string)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:17:25.162565Z","iopub.execute_input":"2024-03-02T05:17:25.163178Z","iopub.status.idle":"2024-03-02T05:17:25.170168Z","shell.execute_reply.started":"2024-03-02T05:17:25.163146Z","shell.execute_reply":"2024-03-02T05:17:25.169295Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"python, django, nodejs, c  c, sql, cicd, javascript, big data hadoop, spark, cloud aws, gcp\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LSTM model trained on O-NET dataset\n\n### **Standardization Challenge**: \nEnhance the \"ResumeRevealer\" to standardize different job titles and occupations against the O-NET database, ensuring a consistent taxonomy across parsed resumes.","metadata":{}},{"cell_type":"markdown","source":"This Python script uses TensorFlow and Keras to build and train a Bidirectional LSTM (Long Short-Term Memory) model for a multiclass classification task. Here's a brief description of the code:\n\n**Libraries Used**:\n\n**pandas**: Used for data manipulation and analysis.\n\n**tensorflow**: An open-source machine learning library.\n\n**Sequential, Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization**: Keras layers for constructing neural networks.\n\n**Tokenizer**: Tokenizes input text into sequences.\n\n**pad_sequences**: Pads sequences to ensure uniform length.\n\n**train_test_split**: Splits the dataset into training and testing sets.\nData Loading:\n\nReads a CSV file containing data related to technical skills from O*NET (Occupational Information Network) into a Pandas DataFrame (df).\n\n**Data Preprocessing:**\n\nExtracts relevant features ('title', 'commodity_title', 'tech_skill') and target labels ('O*NET-SOC Code') from the DataFrame.\nTokenizes and sequences the technical skills using the Tokenizer class.\nConverts job profiles to one-hot encoding for multiclass classification.\nSplits the dataset into training and testing sets.\nModel Architecture:\n\nBuilds a sequential LSTM model with bidirectional layers.\n\n**Embedding layer**: Maps words to vectors of fixed size.\n\n**Bidirectional LSTM layers**: Captures information from both directions in the sequence.\n\n**BatchNormalization**: Normalizes layer inputs.\n\nDense layers with ReLU and softmax activations for classification.\nModel Compilation and Training:\n\nCompiles the model using the Adam optimizer and categorical crossentropy loss.\nPrints the model summary.\nTrains the model using the training data, with 25 epochs and a batch size of 32.\nValidates the model on a small subset (1%) of the training data.\nModel Evaluation:\n\nEvaluates the trained model on the testing set.\nPrints the test accuracy.\nOverall, this script demonstrates the construction, training, and evaluation of a Bidirectional LSTM model for the classification of O*NET-SOC codes based on job profiles and technical skills.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout, BatchNormalization\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\n# Load your data into a DataFrame\ndf = pd.read_csv(\"/kaggle/input/tech-skill-dataset/TECH_SKILLS.csv\")\n\n# Define features (X) and target labels (Y)\nX_train = df[['title', 'commodity_title', 'tech_skill']]\ny_train = df['O*NET-SOC Code']\n\n# Tokenize words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train['tech_skill'])\ntotal_words = len(tokenizer.word_index) + 1\n\n# Convert words to sequences\nsequences = tokenizer.texts_to_sequences(X_train['tech_skill'])\nmax_sequence_length = max([len(seq) for seq in sequences])\npadded_sequences = pad_sequences(sequences, padding='post')\n\n# Convert job profiles to one-hot encoding\nlabels = pd.get_dummies(y_train)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=47)\n\n# Build the LSTM model\nonet_model = Sequential()\nonet_model.add(Embedding(input_dim=total_words, output_dim=100))\nonet_model.add(Bidirectional(LSTM(128, return_sequences=True)))\nonet_model.add(BatchNormalization())\nonet_model.add(Bidirectional(LSTM(64)))\nonet_model.add(BatchNormalization())\nonet_model.add(Dense(64, activation='relu'))\nonet_model.add(Dense(len(labels.columns), activation='softmax'))\n\n# Compile the model\nonet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Print the model summary\nonet_model.summary()\n\n# Train the model\nonet_model.fit(X_train, y_train, epochs=25, batch_size=32, validation_split=0.01)\n\n# Evaluate the model\nloss, accuracy = onet_model.evaluate(X_test, y_test)\nprint(f'Test Accuracy: {accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:11:02.173577Z","iopub.execute_input":"2024-03-02T05:11:02.174342Z","iopub.status.idle":"2024-03-02T05:14:15.973408Z","shell.execute_reply.started":"2024-03-02T05:11:02.174310Z","shell.execute_reply":"2024-03-02T05:14:15.972496Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_1\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_2           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ batch_normalization_3           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.0098 - loss: 6.4448 - val_accuracy: 0.0128 - val_loss: 6.2727\nEpoch 2/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.0298 - loss: 5.7701 - val_accuracy: 0.0340 - val_loss: 5.9588\nEpoch 3/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.0717 - loss: 5.2539 - val_accuracy: 0.0298 - val_loss: 5.8133\nEpoch 4/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.1261 - loss: 4.7434 - val_accuracy: 0.0468 - val_loss: 5.9168\nEpoch 5/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 11ms/step - accuracy: 0.1753 - loss: 4.3896 - val_accuracy: 0.0468 - val_loss: 6.0459\nEpoch 6/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2054 - loss: 4.1539 - val_accuracy: 0.0553 - val_loss: 6.0623\nEpoch 7/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2279 - loss: 3.9809 - val_accuracy: 0.0426 - val_loss: 6.5102\nEpoch 8/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2446 - loss: 3.8685 - val_accuracy: 0.0468 - val_loss: 6.3636\nEpoch 9/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.2614 - loss: 3.7322 - val_accuracy: 0.0511 - val_loss: 6.5108\nEpoch 10/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2653 - loss: 3.6688 - val_accuracy: 0.0383 - val_loss: 6.9709\nEpoch 11/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2680 - loss: 3.6170 - val_accuracy: 0.0596 - val_loss: 6.9799\nEpoch 12/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2719 - loss: 3.5629 - val_accuracy: 0.0383 - val_loss: 7.1817\nEpoch 13/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.2807 - loss: 3.5031 - val_accuracy: 0.0426 - val_loss: 7.6398\nEpoch 14/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2799 - loss: 3.4894 - val_accuracy: 0.0511 - val_loss: 7.5435\nEpoch 15/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2805 - loss: 3.4424 - val_accuracy: 0.0340 - val_loss: 7.8752\nEpoch 16/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2783 - loss: 3.4178 - val_accuracy: 0.0340 - val_loss: 7.7825\nEpoch 17/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2812 - loss: 3.3898 - val_accuracy: 0.0426 - val_loss: 7.5273\nEpoch 18/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.2837 - loss: 3.3685 - val_accuracy: 0.0511 - val_loss: 8.3690\nEpoch 19/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2816 - loss: 3.3551 - val_accuracy: 0.0468 - val_loss: 8.3324\nEpoch 20/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2927 - loss: 3.3062 - val_accuracy: 0.0383 - val_loss: 8.6512\nEpoch 21/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2867 - loss: 3.3105 - val_accuracy: 0.0511 - val_loss: 8.1158\nEpoch 22/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.2894 - loss: 3.2830 - val_accuracy: 0.0553 - val_loss: 8.4251\nEpoch 23/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2888 - loss: 3.2707 - val_accuracy: 0.0426 - val_loss: 8.5499\nEpoch 24/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2919 - loss: 3.2641 - val_accuracy: 0.0468 - val_loss: 8.1751\nEpoch 25/25\n\u001b[1m727/727\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 10ms/step - accuracy: 0.2925 - loss: 3.2367 - val_accuracy: 0.0468 - val_loss: 8.9297\n\u001b[1m184/184\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.0396 - loss: 8.9105\nTest Accuracy: 3.83%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Mapping predicted code to Job title","metadata":{}},{"cell_type":"code","source":"tech_skill_df = pd.read_csv('/kaggle/input/tech-skill-dataset/TECH_SKILLS.csv')\nqnet_to_title = dict(zip(tech_skill_df['O*NET-SOC Code'], tech_skill_df['title']))","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:17:31.475298Z","iopub.execute_input":"2024-03-02T05:17:31.475672Z","iopub.status.idle":"2024-03-02T05:17:31.534987Z","shell.execute_reply.started":"2024-03-02T05:17:31.475643Z","shell.execute_reply":"2024-03-02T05:17:31.534218Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"new_data = [skills_string]\n\nnew_sequences = tokenizer.texts_to_sequences(new_data)\nnew_padded_sequences = pad_sequences(new_sequences, maxlen=20, padding='post')\npredictions = onet_model.predict(new_padded_sequences)\n\n# Convert predictions to class labels\npredicted_labels = [qnet_to_title[labels.columns[np.argmax(prediction)]] for prediction in predictions]\n\nprint(\"Predicted Job Profiles:\", predicted_labels)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T05:18:01.257581Z","iopub.execute_input":"2024-03-02T05:18:01.258263Z","iopub.status.idle":"2024-03-02T05:18:01.328290Z","shell.execute_reply.started":"2024-03-02T05:18:01.258225Z","shell.execute_reply":"2024-03-02T05:18:01.327468Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nPredicted Job Profiles: ['Web Developers']\n","output_type":"stream"}]}]}